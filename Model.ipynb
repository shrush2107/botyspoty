{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import xgboost,textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF=pd.read_csv(\"clean.csv\",encoding='mac_roman', na_filter=True, na_values='[]')\n",
    "trainDF.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>Labels</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_tokens_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Turns out the cop did not even bake that fucki...</td>\n",
       "      <td>1</td>\n",
       "      <td>turns cop even bake fucking cake</td>\n",
       "      <td>['turns', 'cop', 'even', 'bake', 'fucking', 'c...</td>\n",
       "      <td>['turn', 'cop', 'even', 'bake', 'fucking', 'ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@realDonaldTrump Hey Don! You up? Donnnnaaald!...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey donnnnaaald awake</td>\n",
       "      <td>['hey', 'donnnnaaald', 'awake']</td>\n",
       "      <td>['hey', 'donnnnaaald', 'awake']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Man drowned in Nichols home is 5th hurricane d...</td>\n",
       "      <td>1</td>\n",
       "      <td>man drowned nichols home th hurricane death sc...</td>\n",
       "      <td>['man', 'drowned', 'nichols', 'home', 'th', 'h...</td>\n",
       "      <td>['man', 'drowned', 'nichols', 'home', 'th', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>It‚Äö√Ñ√¥s time to phase immigration down so w...</td>\n",
       "      <td>1</td>\n",
       "      <td>time phase immigration phase americans back</td>\n",
       "      <td>['time', 'phase', 'immigration', 'phase', 'ame...</td>\n",
       "      <td>['time', 'phase', 'immigration', 'phase', 'ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calm is a superpower.  ‚Äö√∫√•  https://t.co/B...</td>\n",
       "      <td>1</td>\n",
       "      <td>calm superpower</td>\n",
       "      <td>['calm', 'superpower']</td>\n",
       "      <td>['calm', 'superpower']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83413</th>\n",
       "      <td>83413</td>\n",
       "      <td>SWING STATE SWING: New poll shows Trump +4 in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>swing state swing new poll shows trump pa</td>\n",
       "      <td>['swing', 'state', 'swing', 'new', 'poll', 'sh...</td>\n",
       "      <td>['swing', 'state', 'swing', 'new', 'poll', 'sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83414</th>\n",
       "      <td>83414</td>\n",
       "      <td>BLOOP! \\| On Black Women As Birth Of A Nation‚...</td>\n",
       "      <td>1</td>\n",
       "      <td>bloop black women birth nations scapegoats rol...</td>\n",
       "      <td>['bloop', 'black', 'women', 'birth', 'nations'...</td>\n",
       "      <td>['bloop', 'black', 'woman', 'birth', 'nation',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83415</th>\n",
       "      <td>83415</td>\n",
       "      <td>Bob Dylan wins 2016 Nobel Prize in literature ...</td>\n",
       "      <td>1</td>\n",
       "      <td>bob dylan wins nobel prize literature</td>\n",
       "      <td>['bob', 'dylan', 'wins', 'nobel', 'prize', 'li...</td>\n",
       "      <td>['bob', 'dylan', 'win', 'nobel', 'prize', 'lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83416</th>\n",
       "      <td>83416</td>\n",
       "      <td>ha scream, annoyed, screaming, enough, yell, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>ha scream annoyed screaming enough yell stop a...</td>\n",
       "      <td>['ha', 'scream', 'annoyed', 'screaming', 'enou...</td>\n",
       "      <td>['ha', 'scream', 'annoyed', 'screaming', 'enou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83417</th>\n",
       "      <td>83417</td>\n",
       "      <td>who needs help finding a job?</td>\n",
       "      <td>1</td>\n",
       "      <td>needs help finding job</td>\n",
       "      <td>['needs', 'help', 'finding', 'job']</td>\n",
       "      <td>['need', 'help', 'finding', 'job']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82661 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            content  Labels  \\\n",
       "0               0  Turns out the cop did not even bake that fucki...       1   \n",
       "1               1  @realDonaldTrump Hey Don! You up? Donnnnaaald!...       0   \n",
       "2               2  Man drowned in Nichols home is 5th hurricane d...       1   \n",
       "3               3  It‚Äö√Ñ√¥s time to phase immigration down so w...       1   \n",
       "4               4  Calm is a superpower.  ‚Äö√∫√•  https://t.co/B...       1   \n",
       "...           ...                                                ...     ...   \n",
       "83413       83413  SWING STATE SWING: New poll shows Trump +4 in ...       1   \n",
       "83414       83414  BLOOP! \\| On Black Women As Birth Of A Nation‚...       1   \n",
       "83415       83415  Bob Dylan wins 2016 Nobel Prize in literature ...       1   \n",
       "83416       83416  ha scream, annoyed, screaming, enough, yell, s...       1   \n",
       "83417       83417                      who needs help finding a job?       1   \n",
       "\n",
       "                                              text_clean  \\\n",
       "0                       turns cop even bake fucking cake   \n",
       "1                                  hey donnnnaaald awake   \n",
       "2      man drowned nichols home th hurricane death sc...   \n",
       "3            time phase immigration phase americans back   \n",
       "4                                        calm superpower   \n",
       "...                                                  ...   \n",
       "83413          swing state swing new poll shows trump pa   \n",
       "83414  bloop black women birth nations scapegoats rol...   \n",
       "83415              bob dylan wins nobel prize literature   \n",
       "83416  ha scream annoyed screaming enough yell stop a...   \n",
       "83417                             needs help finding job   \n",
       "\n",
       "                                             text_tokens  \\\n",
       "0      ['turns', 'cop', 'even', 'bake', 'fucking', 'c...   \n",
       "1                        ['hey', 'donnnnaaald', 'awake']   \n",
       "2      ['man', 'drowned', 'nichols', 'home', 'th', 'h...   \n",
       "3      ['time', 'phase', 'immigration', 'phase', 'ame...   \n",
       "4                                 ['calm', 'superpower']   \n",
       "...                                                  ...   \n",
       "83413  ['swing', 'state', 'swing', 'new', 'poll', 'sh...   \n",
       "83414  ['bloop', 'black', 'women', 'birth', 'nations'...   \n",
       "83415  ['bob', 'dylan', 'wins', 'nobel', 'prize', 'li...   \n",
       "83416  ['ha', 'scream', 'annoyed', 'screaming', 'enou...   \n",
       "83417                ['needs', 'help', 'finding', 'job']   \n",
       "\n",
       "                                       text_tokens_lemma  \n",
       "0      ['turn', 'cop', 'even', 'bake', 'fucking', 'ca...  \n",
       "1                        ['hey', 'donnnnaaald', 'awake']  \n",
       "2      ['man', 'drowned', 'nichols', 'home', 'th', 'h...  \n",
       "3      ['time', 'phase', 'immigration', 'phase', 'ame...  \n",
       "4                                 ['calm', 'superpower']  \n",
       "...                                                  ...  \n",
       "83413  ['swing', 'state', 'swing', 'new', 'poll', 'sh...  \n",
       "83414  ['bloop', 'black', 'woman', 'birth', 'nation',...  \n",
       "83415  ['bob', 'dylan', 'win', 'nobel', 'prize', 'lit...  \n",
       "83416  ['ha', 'scream', 'annoyed', 'screaming', 'enou...  \n",
       "83417                 ['need', 'help', 'finding', 'job']  \n",
       "\n",
       "[82661 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=trainDF['text_tokens_lemma']\n",
    "Y=trainDF['Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['turn', 'cop', 'even', 'bake', 'fucking', 'ca...\n",
       "1                          ['hey', 'donnnnaaald', 'awake']\n",
       "2        ['man', 'drowned', 'nichols', 'home', 'th', 'h...\n",
       "3        ['time', 'phase', 'immigration', 'phase', 'ame...\n",
       "4                                   ['calm', 'superpower']\n",
       "                               ...                        \n",
       "83413    ['swing', 'state', 'swing', 'new', 'poll', 'sh...\n",
       "83414    ['bloop', 'black', 'woman', 'birth', 'nation',...\n",
       "83415    ['bob', 'dylan', 'win', 'nobel', 'prize', 'lit...\n",
       "83416    ['ha', 'scream', 'annoyed', 'screaming', 'enou...\n",
       "83417                   ['need', 'help', 'finding', 'job']\n",
       "Name: text_tokens_lemma, Length: 82661, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "83413    1\n",
       "83414    1\n",
       "83415    1\n",
       "83416    1\n",
       "83417    1\n",
       "Name: Labels, Length: 82661, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(X, Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66128,) (16533,) (66128,) (16533,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,valid_x.shape,train_y.shape,valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20960    ['explanation', 'expected', 'im', 'glad', 'won...\n",
       "6921     ['mainstream', 'medium', 'doesnt', 'hold', 'hi...\n",
       "47761    ['tune', 'ogradio', 'w', 'radio', 'pm', 'et', ...\n",
       "54465    ['trump', 'tax', 'record', 'obtained', 'time',...\n",
       "46666    ['welp', 'proof', 'rigged', 'debate', 'schedul...\n",
       "                               ...                        \n",
       "41920    ['u', 'lived', 'thru', 'year', 'scandal', 'kno...\n",
       "11339    ['nice', 'afternoon', 'vega', 'meeting', 'fan'...\n",
       "28839    ['chicago', 'teacher', 'union', 'vote', 'appro...\n",
       "31908    ['budweiser', 'study', 'show', 'american', 'wa...\n",
       "82785    ['thank', 'much', 'shoutout', 'lovetrumpshate'...\n",
       "Name: text_tokens_lemma, Length: 66128, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text_tokens_lemma'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<66128x44572 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 524711 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16533x44572 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 131962 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrus\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=8000)\n",
    "tfidf_vect.fit(trainDF['text_tokens_lemma'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=8000)\n",
    "tfidf_vect_ngram.fit(trainDF['text_tokens_lemma'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=8000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text_tokens_lemma'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# load the pre-trained word-embedding vectors \\nembeddings_index = {}\\nfor i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\\n    values = line.split()\\n    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\\n\\n# create a tokenizer \\ntoken = text.Tokenizer()\\ntoken.fit_on_texts(trainDF['text_tokens_lemma'])\\nword_index = token.word_index\\n\\n# convert text to sequence of tokens and pad them to ensure equal length vectors \\ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\\n\\n# create token-embedding mapping\\nembedding_matrix = numpy.zeros((len(word_index) + 1, 300))\\nfor word, i in word_index.items():\\n    embedding_vector = embeddings_index.get(word)\\n    if embedding_vector is not None:\\n        embedding_matrix[i] = embedding_vector\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text_tokens_lemma'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.9357648339684268\n",
      "NB, WordLevel TF-IDF:  0.937276961229057\n",
      "NB, N-Gram Vectors:  0.9372164761386318\n",
      "NB, CharLevel Vectors:  0.9309260267344099\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.9500998003992016\n",
      "LR, WordLevel TF-IDF:  0.9452005080747595\n",
      "LR, N-Gram Vectors:  0.9363696848726789\n",
      "LR, CharLevel Vectors:  0.9390915139418133\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=500), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=500), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=500), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(max_iter=500), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# RF on Count Vectors\\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\\nprint (\"RF, Count Vectors: \", accuracy)\\n\\n#RF on Word Level TF IDF Vectors\\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\\nprint (\"RF, WordLevel TF-IDF: \", accuracy)\\n\\n# RF on Ngram Level TF IDF Vectors\\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_count)\\nprint (\"RF, Count Vectors: \", accuracy)\\n\\n# RF on Character Level TF IDF Vectors\\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf)\\nprint (\"RF, WordLevel TF-IDF: \", accuracy)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "#RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# RF on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Character Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# SVM on Count Vectors\\naccuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\\nprint (\"SVM, Count Vectors: \", accuracy)\\n\\n# SVM on Word Level TF IDF Vectors\\naccuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\\nprint (\"SVM, WordLevel TF-IDF: \", accuracy)\\n\\n# SVM on Ngram Level TF IDF Vectors\\naccuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_count)\\nprint (\"SVM, Count Vectors: \", accuracy)\\n\\n# SVM on Character Level TF IDF Vectors\\naccuracy = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf)\\nprint (\"SVM, WordLevel TF-IDF: \", accuracy)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# SVM on Count Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM, Count Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_count)\n",
    "print (\"SVM, Count Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf)\n",
    "print (\"SVM, WordLevel TF-IDF: \", accuracy)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def create_rnn_lstm():\\n    # Add an Input Layer\\n    input_layer = layers.Input((70, ))\\n\\n    # Add the word embedding Layer\\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\\n\\n    # Add the LSTM Layer\\n    lstm_layer = layers.LSTM(100)(embedding_layer)\\n\\n    # Add the output Layers\\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\\n\\n    # Compile the model\\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\\n    model.compile(optimizer=optimizers.Adam(), loss=\\'binary_crossentropy\\')\\n    \\n    return model\\n\\nclassifier = create_rnn_lstm()\\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\\nprint (\"RNN-LSTM, Word Embeddings\",  accuracy)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST MODEL (selected on the basis of accuracy on validation set and time taken to train the model)\n",
    "\n",
    "LR on Count Vectors is the best model WITH ACCURACY OF approx 94.8% on validation set\n",
    "\n",
    "Also SVM on tfidF gives an ACCURACY OF 94.82% but takes longer time to train an predict \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  808   648]\n",
      " [  177 14900]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9500998003992016"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "loreg=linear_model.LogisticRegression(max_iter=200)# initialize the model\n",
    "loreg.fit(xtrain_count, train_y) # fit he model\n",
    "y_pred=loreg.predict(xvalid_count) # now predict\n",
    "cm = confusion_matrix(valid_y, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(valid_y, y_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
